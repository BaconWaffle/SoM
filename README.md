# SoM

> We present Set-of-Mark (SoM), simply overlaying a number of spatial and speakable marks on the images, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.

![teaser_github](https://github.com/microsoft/SoM/assets/11957155/e4720105-b4b2-40c0-9303-2d8f1cb27d91)

![method5_xyz](https://github.com/microsoft/SoM/assets/34880758/3e8257d6-fd88-454d-a892-0ea2c5b478a0)
![method4_xyz](https://github.com/microsoft/SoM/assets/34880758/a9cddc47-f975-4991-b35a-72c50813c092)
![method3_xyz](https://github.com/microsoft/SoM/assets/34880758/2443572b-995a-4f29-95df-3e3fc0f510d6)

![task_examples](https://github.com/microsoft/SoM/assets/34880758/5676ee40-a051-404f-8eed-74fe87020916)
