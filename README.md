# SoM

> We present Set-of-Mark (SoM), simply overlaying a number of spatial and speakable marks on the images, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.

![teaser_github](https://github.com/microsoft/SoM/assets/11957155/e4720105-b4b2-40c0-9303-2d8f1cb27d91)
![method2_xyz](https://github.com/microsoft/SoM/assets/34880758/32a269c4-8465-4eaf-aa90-48e9534649d9)

![method4_xyz](https://github.com/microsoft/SoM/assets/34880758/a9cddc47-f975-4991-b35a-72c50813c092)
![method3_xyz](https://github.com/microsoft/SoM/assets/34880758/2443572b-995a-4f29-95df-3e3fc0f510d6)

![task_examples](https://github.com/microsoft/SoM/assets/34880758/5676ee40-a051-404f-8eed-74fe87020916)
![use_case_tooluse](https://github.com/microsoft/SoM/assets/34880758/00e5c89b-dbba-4755-a39c-056e229f5c18)
![use_case_compare](https://github.com/microsoft/SoM/assets/34880758/13d1fc38-e605-41c0-8b54-009d0ce98e1e)
![main_results](https://github.com/microsoft/SoM/assets/34880758/722ac979-6c7f-4740-9625-cac38060e0ad)
